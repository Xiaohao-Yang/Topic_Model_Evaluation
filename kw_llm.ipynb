{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c1114e-4063-4cac-b86d-c4543b916a92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T04:54:43.946244Z",
     "start_time": "2025-01-10T04:54:20.792480Z"
    }
   },
   "source": [
    "## Generate keywords for test documents from an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d20715-95a7-419e-a192-94c1345ac7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/xiaohao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/home/xiaohao/anaconda3/envs/walm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from walm import generate_keywords\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import scipy.io as sio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be45d3a2-b01d-4249-ba9a-33b32e3c5347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "dataset = '20News'\n",
    "data_dict = sio.loadmat('datasets/%s/data.mat' % dataset)\n",
    "test_doc = data_dict['test_text'].tolist()\n",
    "test_doc = [doc[0][0].strip() for doc in test_doc]\n",
    "\n",
    "# take 10 documents as an example\n",
    "test_doc = test_doc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68dc49d9aee6082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# load an llm model, we support the following LLMs in current version\n",
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "# model_name = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "# model_name = 'microsoft/Phi-3-mini-128k-instruct'\n",
    "# model_name = '01-ai/Yi-1.5-9B-Chat'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             trust_remote_code=True,\n",
    "                                             torch_dtype=torch.float16\n",
    "                                             ).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f230ca-4b28-4d27-8eda-18a96d64cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLM Inference ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate keywords\n",
    "save_path = 'test_opt.txt'\n",
    "outputs = generate_keywords(model, tokenizer, test_doc, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f3742fc-b005-47d9-a667-39be1ca4d4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Words': ['Harvard', 'Graphics', 'Windows', 'Sale', 'Price']}\n",
      "{'Words': ['Abortion', 'Rights', 'Case', 'Good', 'Pro']}\n",
      "{'Words': ['mound', 'season', 'strike', 'zone', 'seventies']}\n",
      "{'Words': ['PlusMinus', 'Players', 'Role', 'Time', 'Jagr']}\n",
      "{'Words': ['Efficiency', 'X11', 'Clients', 'Performance', 'XRemote']}\n",
      "{'Words': ['Windows', 'Hardware', 'Interrupts', 'DOS', 'UART']}\n",
      "{'Words': ['train', 'station', 'cities', 'drive', 'airport']}\n",
      "{'Words': ['IDE', 'Drive', 'Error', 'Format', 'Software']}\n",
      "{'Words': ['Projector', 'Super', '8mm', 'Sound', 'ForSale']}\n",
      "{'Words': ['Society', 'Life', 'Killing', 'Value', 'Children']}\n"
     ]
    }
   ],
   "source": [
    "for item in outputs:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef31471-6e5f-40c9-99e3-6ac94ba55399",
   "metadata": {},
   "source": [
    "The output will also be saved in your defined .txt file in JSON format:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
